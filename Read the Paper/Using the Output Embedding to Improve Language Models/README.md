[Paper](https://arxiv.org/abs/1608.05859)

# Using the Output Embedding to Improve Language Models

## 1) Summary
 Our methods weight tying leads to a significant reducing in preplexity, as we are able to show on a variety of neural network language models.
We show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their preformance.

## 2) Evaluation
 1. Novel Idea => Normal
 2. Good Motivation => Normal
 3. Easy to Read => Good
 4. Reproducible => Good
 5. Convincing Results => Good

 - Points in favor
  1. the details description

## 3) why not
  1. popular ideas

