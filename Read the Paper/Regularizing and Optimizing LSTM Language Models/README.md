# Regularizing and Optimizing LSTM Language Models

[Link](https://arxiv.org/abs/1708.02182)

## 1) Summary
  In this paper, we consider the specific problem of word-level language modeling, and question answering. Finally, we explore the use of a neural cache in conjunction with our proposed model and show that thos further improves the performance.
  (BPTT,ASGD,DropConnect) thus attaining an even lower state-of-the-art perplexity.
  While the regularization and optimization strategies proposed are demonstrated on the task of language modeling, we anticipate that they would be generally applicable across other sequence learning tasks.
  
## 2) Evaluation
  1. Novel Idea => Normal(Try the many method)
  2. Good Motivation => Good(Summary the various ways)
  3. Easy to Read => Good
  4. Reproducible => Good
  5. Convincing Results => Good
  
  - Points in favor
    1. brief description
