

## LINK : https://arxiv.org/abs/1406.1078

매우 간단한 논문 리뷰. 위와 같은 논문은 GRU 모델을 이용해 ENCODER-DECODER 모델을 개선시켰다. 알다시피 LSTM이 아니다 보니 속도와 메모리가 덜 사용된다. 또한 정확도도 높다. 이뿐만 아닌 어텐션 모델처럼 다양한 길이의 sequence가 나온다는 점에서 다른 논문들과 큰 차이가 있다. 저자는 깊은 데이터 분석을 통해 모델의 훌륭함을 보인다.

# 1) 간단한 요약(Summary)
 Qualitatively, we show that the proposed model learns a sematically and syntatically meaningful representation of linguistic phrase. Our qualitative analysis of the trained model shows that it indeed captures the linguistic regularities in multiple levels i.e. at the world level as well as phrase level. The proposed architecture has large potential for further improvement and analysis.
 
# 2) 평가(Evaluation)
  1. Novel Idea =>  Normal.
  2. Good Motivation => Good (Improve the encoder-decoder model.)
  3. Easy to Read => Good
  4. Reproducible => --
  5. Convincing results => Good
  
# - Points in Favor
   1. Word representation
   2. Variable length sequence
   3. Use the GRU model and it makes more better.
 
# 3) 부족한점 or 좋은점
   1....
